---
title: "STAC create item "
format: html
engine: knitr
---

```{r env-activate}
# Auto-install reticulate if not present
if (!require("reticulate", quietly = TRUE)) {
  pak::pkg_install("reticulate")
  library(reticulate)
}

reticulate::use_condaenv("stac-catalog", required = TRUE)
# reticulate::py_config()

```


```{python import-modules}
import pathlib
import pystac
import rio_stac
import concurrent.futures
import subprocess
import pandas as pd
import os
import glob
from tqdm import tqdm
from pystac import Link, RelType, Item
```

```{python date-extract}
import re

def date_extract_after_utm(s):
    match = re.search(r'_utm\d{1,2}_([0-9]{4,8})', s)
    return match.group(1) if match else None

# this is more flexible 
def date_extract_from_path(s):
    # Step 1: Try to extract YYYYMMDD or YYYY after _utmXX_
    match = re.search(r'_utm\d{1,2}_([0-9]{4,8})', s)
    if match:
        val = match.group(1)
        if val.isdigit():
            year = int(val[:4])
            if 2000 <= year <= 2050:
                return val

    # Step 2: Fallback: look for /YYYY/ in the path
    fallback = re.search(r'/([2][0-9]{3})/', s)
    if fallback:
        year = int(fallback.group(1))
        if 2000 <= year <= 2050:
            return str(year)

    return None


from datetime import datetime, timezone

def datetime_parse_item(s):
    if s is None:
        return None
    if len(s) == 8:
        return datetime.strptime(s, "%Y%m%d").replace(tzinfo=timezone.utc)
    elif len(s) == 4:
        return datetime.strptime(s, "%Y").replace(tzinfo=timezone.utc)
    return None


```


```{python stac-item-create}
# =============================================================================
# Configuration
# =============================================================================

# Test mode flags - set to True for development testing
test_only = False  # PRODUCTION: Reprocess all 2,245 albers items with href fix
test_number_items = 100  # Only used when test_only=True

# Incremental mode - set to True to process only new URLs from data/urls_new.txt
incremental = False

# Reprocess invalid items mode - set to True to re-process items from urls_invalid_items.txt
reprocess_invalid = True  # Re-process invalid items with href fix

# Define paths - test_only controls dev vs prod output
if test_only:
    # DEV: Safe testing environment, isolated from production
    path_local = "/Users/airvine/Projects/gis/stac_dem_bc/stac/dev/stac_dem_bc"
else:
    # PROD: Production output that gets uploaded to S3
    path_local = "/Users/airvine/Projects/gis/stac_dem_bc/stac/prod/stac_dem_bc"

path_collection = f"{path_local}/collection.json"
path_s3_stac = "https://stac-dem-bc.s3.amazonaws.com"
path_s3_json = f"{path_s3_stac}/collection.json"
path_s3 = "https://nrs.objectstore.gov.bc.ca/gdwuts"
path_results_csv = "data/stac_geotiff_checks.csv"

print(f"Mode: {'TEST (dev output)' if test_only else 'PRODUCTION (prod output)'}")
print(f"Output directory: {path_local}")

# Load base collection
collection = pystac.Collection.from_file(path_collection)
collection.set_self_href(path_s3_json)

# Load list of image paths (from appropriate file based on mode)
if reprocess_invalid:
    urls_file = "data/urls_invalid_items.txt"
    mode_desc = "reprocess_invalid"
elif incremental:
    urls_file = "data/urls_new.txt"
    mode_desc = "incremental"
else:
    urls_file = "data/urls_list.txt"
    mode_desc = "full"

with open(urls_file) as f:
    path_items = f.read().splitlines()

# Determine URLs to check
if test_only:
    urls_to_check = path_items[:test_number_items]
else:
    urls_to_check = path_items

print(f"Processing {len(urls_to_check)} URLs (mode={mode_desc}, test_only={test_only})")

# Handle existing item links based on mode
# WHY: Test mode normally clears items for clean slate, BUT incremental mode needs
#      to keep existing items to append new ones. If both flags are set, incremental
#      takes priority (we're testing the append behavior).
if reprocess_invalid:
    # Reprocess invalid mode: Keep all existing items, we're updating specific JSON files
    # Count existing items for reporting
    existing_item_count = len([l for l in collection.links if l.rel == 'item'])
    print(f"Reprocess invalid mode: Updating {len(path_items)} items (collection has {existing_item_count} total items)")
elif test_only and not incremental:
    # Test mode (non-incremental): Clear for clean test run
    # Remove all item links (keeps other links like self, root, etc.)
    collection.links = [link for link in collection.links if link.rel != 'item']
    print("Test mode: Cleared existing item links from collection")

    # WHY: Also delete old item JSON files to prevent accumulation across test runs.
    #      Without this, old JSON files remain even though collection.links is cleared.
    old_jsons = glob.glob(f"{path_local}/*-*.json")  # Match item pattern (not collection.json)
    if old_jsons:
        for json_file in old_jsons:
            os.remove(json_file)
        print(f"Test mode: Deleted {len(old_jsons)} old item JSON files")
elif incremental:
    # Incremental mode: Keep existing items, we're appending
    # Count and report how many items we're starting with
    existing_item_count = len([l for l in collection.links if l.rel == 'item'])
    print(f"Incremental mode: Appending to collection with {existing_item_count} existing items")

# =============================================================================
# Pre-Validation: Check GeoTIFF and COG Status
# =============================================================================

# Load existing validation cache if it exists
if os.path.exists(path_results_csv):
    df_existing = pd.read_csv(path_results_csv)
    existing_urls = set(df_existing["url"])
    print(f"Loaded {len(df_existing)} existing validation results")
else:
    df_existing = pd.DataFrame(columns=["url", "is_geotiff", "is_cog"])
    existing_urls = set()
    print("No existing validation cache found, will validate all URLs")

# Determine which URLs need validation (always use cache)
urls_to_validate = [url for url in urls_to_check if url not in existing_urls]
print(f"{len(urls_to_validate)} URLs need validation ({len(urls_to_check) - len(urls_to_validate)} already cached)")

def check_geotiff_cog(url: str) -> dict:
    """
    Validate GeoTIFF and COG status in one step using rio cogeo validate.

    Returns dict with url, is_geotiff (readable), and is_cog (cloud-optimized) status.
    """
    try:
        result = subprocess.run(
            ["rio", "cogeo", "validate", f"/vsicurl/{url}"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=False
        )
        output = result.stdout.decode() + result.stderr.decode()
        return {
            "url": url,
            "is_geotiff": "is NOT a valid cloud optimized GeoTIFF" in output or "is a valid cloud optimized GeoTIFF" in output,
            "is_cog": "is a valid cloud optimized GeoTIFF" in output
        }
    except FileNotFoundError:
        raise RuntimeError("`rio cogeo` is not installed or not in PATH. Install with: pip install rio-cogeo")

# Run validation in parallel if there are URLs to validate
if urls_to_validate:
    print(f"Validating {len(urls_to_validate)} GeoTIFFs...")
    with concurrent.futures.ThreadPoolExecutor() as executor:
        new_results = list(tqdm(executor.map(check_geotiff_cog, urls_to_validate),
                               total=len(urls_to_validate),
                               desc="Validating GeoTIFFs"))

    # Combine and save
    df_new = pd.DataFrame(new_results)
    df_all = pd.concat([df_existing, df_new], ignore_index=True) if len(df_existing) > 0 else df_new
    df_all.to_csv(path_results_csv, index=False)
    print(f"Validation complete. Saved {len(df_all)} results to {path_results_csv}")
else:
    df_all = df_existing
    print("No new URLs to validate, using existing cache")

# Create lookup dictionary for fast access during item creation
results_lookup = {
    row["url"].replace("https:/", "https://"): {"is_geotiff": row["is_geotiff"], "is_cog": row["is_cog"]}
    for _, row in df_all.iterrows()
}

# =============================================================================
# Parallel Item Creation
# =============================================================================

def process_item(path_item: str) -> dict | None:
    """
    Process a single GeoTIFF URL to create a STAC item.

    Returns dict with item_id and item object, or None if processing fails.
    """
    # Fix malformed URLs (single slash after https:) without breaking correct ones
    if path_item.startswith("https:/") and not path_item.startswith("https://"):
        href_item = path_item.replace("https:/", "https://", 1)
    else:
        href_item = path_item
    check = results_lookup.get(href_item)

    # Skip unreadable GeoTIFFs
    if check is None or not check["is_geotiff"]:
        print(f"Skipping unreadable GeoTIFF: {href_item}")
        return None

    item_id = path_item[len(path_s3):].lstrip("/").replace("/", "-").removesuffix(".tif")

    # Extract datetime from path, use placeholder if not found
    date_str = date_extract_from_path(path_item)
    datetime_is_unknown = False

    if date_str:
        item_time = datetime_parse_item(date_str)
    else:
        # Placeholder datetime for items where date cannot be extracted
        # Common for albers10k2m dataset - see issue #12
        item_time = datetime(2000, 1, 1, tzinfo=timezone.utc)
        datetime_is_unknown = True

    # Set media type based on COG validation results
    media_type = (
        "image/tiff; application=geotiff; profile=cloud-optimized"
        if check["is_cog"] else
        "image/tiff; application=geotiff"
    )

    try:
        item = rio_stac.stac.create_stac_item(
            path_item,
            id=item_id,
            asset_media_type=media_type,
            asset_name='image',
            asset_href=href_item,
            with_proj=True,
            collection=collection.id,
            collection_url=path_s3_json,
            asset_roles=["data"]
        )
        item.datetime = item_time
        item.assets['image'].href = href_item

        # Flag items with unknown datetime for future improvement
        if datetime_is_unknown:
            item.properties["datetime_unknown"] = True

        # Save item JSON locally
        path_item_json = f"{path_local}/{item_id}.json"
        item.save_object(dest_href=path_item_json, include_self_link=False)

        return {
            "id": item_id,
            "item": item
        }
    except Exception as e:
        print(f"Error processing {href_item}: {e}")
        return None

# Run item creation in parallel using threads (avoids rasterio multiprocessing issues)
print(f"Creating STAC items for {len(urls_to_check)} URLs with 32 workers...")
try:
    with concurrent.futures.ThreadPoolExecutor(max_workers=32) as executor:
        results = list(filter(None, tqdm(executor.map(process_item, urls_to_check),
                                        total=len(urls_to_check),
                                        desc="Creating STAC Items")))
except Exception as e:
    print(f"Parallel execution failed: {e}")
    results = []

# Add all valid item JSON hrefs to the collection
if results:
    # WHY: In incremental mode, check for duplicate links before adding.
    #      Without this check, reprocessing the same URLs creates duplicate item links.
    existing_item_hrefs = {link.target for link in collection.links if link.rel == 'item'}

    added_count = 0
    skipped_count = 0
    for result in results:
        item_href = f"{path_s3_stac}/{result['id']}.json"
        if item_href not in existing_item_hrefs:
            collection.add_link(Link(
                rel=RelType.ITEM,
                target=item_href,
                media_type="application/json"
            ))
            added_count += 1
        else:
            skipped_count += 1

    print(f"Successfully created {len(results)} STAC items")
    print(f"Added {added_count} new item links, skipped {skipped_count} duplicates")
else:
    print("No items were created")

# Save the updated collection
collection.save_object(dest_href=path_collection)
print(f"Collection saved to {path_collection}")

# =============================================================================
# Validation
# =============================================================================
# To validate items, use the standalone script:
#   python scripts/validate_stac_items.py --items-dir <path>
# This validates from local JSON files and outputs results to CSV.

```



```{r json-clean, eval= FALSE}
# DELETE ATTACK - USE WITH CARE
# list all the jsonsin the directory so we can delete them
path_base = "/Users/airvine/Projects/gis/stac_dem_bc/stac/prod/stac_dem_bc"
# path_base = "/Users/airvine/Projects/gis/uav_imagery/stac/dev/imagery_uav_bc"

j <- fs::dir_ls(path_base, glob = "*.json", recurse = TRUE)

fs::file_delete(j)

```


