---
title: "STAC create collection "
format: html
engine: knitr
---

```{r env-activate}
# Auto-install reticulate if not present
if (!require("reticulate", quietly = TRUE)) {
  pak::pkg_install("reticulate")
  library(reticulate)
}

reticulate::use_condaenv("stac-catalog", required = TRUE)
# reticulate::py_config()

# Test mode configuration
test_only <- FALSE  # Set to FALSE for production
test_number_items <- 10

cat(sprintf("Mode: %s\n", if(test_only) "TEST" else "PRODUCTION"))

```

```{r s3-get-keys}
fs::dir_create("data")

if (test_only && file.exists("data/urls_list.txt")) {
  # TEST MODE: Reuse existing urls_list.txt (skip slow S3 fetch)
  cat("Test mode: Reusing existing data/urls_list.txt\n")
  keys_clean <- readr::read_lines("data/urls_list.txt")
  cat(sprintf("Loaded %d URLs from cache\n", length(keys_clean)))
} else {
  # PRODUCTION MODE: Fetch fresh keys from BC objectstore
  cat("Fetching fresh keys from BC objectstore...\n")
  keys <- ngr::ngr_s3_keys_get(
    url_bucket = "https://nrs.objectstore.gov.bc.ca/gdwuts",
    prefix = "",
    pattern = c("dem", "*.tif")
  )

  # Remove paths with ( in them
  keys_clean <- keys[!stringr::str_detect(keys, "\\(")]

  # Save to disk
  readr::write_lines(keys_clean, "data/urls_list.txt")
  cat(sprintf("Fetched and saved %d URLs\n", length(keys_clean)))
}

```


```{python import-modules}
import pystac
from pystac import Collection, Extent, SpatialExtent, TemporalExtent
from datetime import datetime
import timeit
```


```{python function-bbox-combined}
import rasterio
from rasterio.warp import transform_bounds
from shapely.geometry import box
from shapely.ops import unary_union
from concurrent.futures import ThreadPoolExecutor, as_completed

def extract_wgs84_bbox(path):
    try:
        with rasterio.open(path) as src:
            bounds = transform_bounds(src.crs, "EPSG:4326", *src.bounds)
            return box(*bounds)
    except Exception as e:
        print(f"âŒ Error processing {path}: {e}")
        return None

def bbox_combined(paths, max_workers=8):
    bboxes = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(extract_wgs84_bbox, path): path for path in paths}
        for future in as_completed(futures):
            result = future.result()
            if result is not None:
                bboxes.append(result)

    if not bboxes:
        raise ValueError("No valid bounding boxes found.")

    union = unary_union(bboxes)
    return union.bounds  # (min lon, min lat, max lon, max lat)

```

```{python test-config}
# =============================================================================
# Configuration
# =============================================================================

# Get test mode flags from R environment (convert to Python types)
test_only = bool(r.test_only)
test_number_items = int(r.test_number_items)

# Define paths - test_only controls dev vs prod output
if test_only:
    # DEV: Safe testing environment, isolated from production
    path_local = "/Users/airvine/Projects/gis/stac_dem_bc/stac/dev/stac_dem_bc"
else:
    # PROD: Production output that gets uploaded to S3
    path_local = "/Users/airvine/Projects/gis/stac_dem_bc/stac/prod/stac_dem_bc"

path_collection = f"{path_local}/collection.json"

print(f"Mode: {'TEST (dev output)' if test_only else 'PRODUCTION (prod output)'}")
print(f"Output: {path_collection}")

# Clean up old test items if in test mode
if test_only:
    import os
    import glob
    old_items = glob.glob(f"{path_local}/*.json")
    old_items = [f for f in old_items if not f.endswith("collection.json")]
    if old_items:
        print(f"Cleaning up {len(old_items)} old test item JSONs...")
        for item_file in old_items:
            os.remove(item_file)
        print("Cleanup complete")

```


```{python temporal-extent}

from datetime import datetime, timezone
import re

def date_extract_from_path(s):
    # Step 1: Try to extract YYYYMMDD or YYYY after _utmXX_
    match = re.search(r'_utm\d{1,2}_([0-9]{4,8})', s)
    if match:
        val = match.group(1)
        if val.isdigit():
            year = int(val[:4])
            if 2000 <= year <= 2050:
                return val

    # Step 2: Fallback: look for /YYYY/ in the path
    fallback = re.search(r'/([2][0-9]{3})/', s)
    if fallback:
        year = int(fallback.group(1))
        if 2000 <= year <= 2050:
            return str(year)

    return None

def datetime_parse_item(s):
    if s is None:
        return None
    if len(s) == 8:
        return datetime.strptime(s, "%Y%m%d").replace(tzinfo=timezone.utc)
    elif len(s) == 4:
        return datetime.strptime(s, "%Y").replace(tzinfo=timezone.utc)
    return None

# Load paths from the same source
with open("data/urls_list.txt") as f:
    path_items = f.read().splitlines()

# Limit to test items if in test mode
if test_only:
    path_items = path_items[:test_number_items]
    print(f"Test mode: Using {len(path_items)} items for extent calculation")

# Extract all valid datetimes
times = [
    datetime_parse_item(date_extract_from_path(p))
    for p in path_items
]

# Filter out invalid values
times = [t for t in times if t is not None]

# Compute min/max
start_time = min(times)
end_time = max(times)

# Create and set temporal extent
temporal_extent = TemporalExtent([[start_time, end_time]])

# Optional: print it for confirmation
print(f"Temporal extent: {start_time.isoformat()} to {end_time.isoformat()}")

```


```{python collection-create}
# path_collection and path_local defined in test-config block above
path_s3_json = "https://stac-dem-bc.s3.amazonaws.com/collection.json"
collection_id = "stac-dem-bc"

# Use path_items from temporal-extent block (already filtered for test mode)
path_cogs = path_items

# =============================================================================
# Spatial Extent Calculation (Phase 1.3 - Issue #4)
# =============================================================================
# Two approaches available:
#
# Option 1: HARDCODED BC BBOX (default - saves ~20 minutes)
#   - BC provincial boundary is stable and well-defined
#   - Slightly less precise (includes areas with no coverage)
#   - Appropriate for provincial datasets like DEM BC
#
# Option 2: CALCULATED from COGs (preserved for other projects)
#   - Exact data extent
#   - Takes ~20 minutes for 22K files
#   - Useful for UAV imagery or datasets with unknown extent
# =============================================================================

# Option 1: Hardcoded BC bbox (default)
bbox = [-140, 48, -114, 60]  # (West, South, East, North)
spatial_extent = SpatialExtent([bbox])
print(f"Using hardcoded BC bbox: {bbox}")

# Option 2: Calculate from COGs (uncomment to use)
# print(f"Calculating spatial extent from {len(path_cogs)} items...")
# bbox = bbox_combined(path_cogs)
# spatial_extent = SpatialExtent([list(bbox)])
# print(f"Calculated bbox: {bbox}")

# Define temporal extent using datetime objects
# temporal_extent = TemporalExtent([[datetime(2011, 9, 14), datetime(2024, 9, 26)]])

# Create the extent object
extent = Extent(spatial=spatial_extent, temporal=temporal_extent)

# Create the STAC Collection
collection = Collection(
    id=collection_id,
    description="A collection of Digital Elevation Models from British Columbia - as served on lidarbc",
    extent=extent,
    license="CC-BY-4.0",
    title=f"Digital Elevation Models from British Columbia - {collection_id}",
    href=path_collection
)


# Save the collection JSON
collection.save(catalog_type=pystac.CatalogType.ABSOLUTE_PUBLISHED)

# Explicitly set the self link
collection.set_self_href(path_s3_json)

# Save locally
collection.save_object(include_self_link=True, dest_href=path_collection)

# Manually force PySTAC to remember the cloud HREF
collection.set_self_href(path_s3_json)

```

Test to see if the collection is valid

```{python}
# stac validate ./bc-uav-collection.json

collection = pystac.Collection.from_file(path_collection)
collection.validate()
```

